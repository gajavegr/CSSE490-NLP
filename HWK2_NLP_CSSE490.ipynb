{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HWK2-NLP-CSSE490.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gajavegr/CSSE490-NLP/blob/main/HWK2_NLP_CSSE490.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2 - Preprocessing and Feature Extraction Solutions\n",
        "\n",
        "\n",
        "\n",
        "In this homework, you will perform \n",
        "\n",
        "  1. **Preprocessing pipeline** (50pts) \n",
        "  2. **Feature extraction** techniques (50pts):\n",
        "    - Count Vectorizer (15pts)\n",
        "    - TF-IDF (20pts)\n",
        "    - CBOW (25pts)\n",
        "\n",
        "<small>Time estimate: ~3-4hrs </small>\n",
        "<hr>\n",
        "<small>Adapted and modified from Natural Language Processing with Classification and Vector Spaces (DeepLearning.AI) </small>\n"
      ],
      "metadata": {
        "id": "xy-byPe54C4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First and Last name: _please complete_\n",
        "\n",
        "Semester: _please complete_"
      ],
      "metadata": {
        "id": "g86i9iQ64i-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 1. Setup\n",
        "\n",
        "Note - you may need to change runtime to GPU first\n",
        "\n",
        "Import the following libraries (install first if missing)"
      ],
      "metadata": {
        "id": "NDsljzCu41n_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzhKB68y34dG"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk \n",
        "# Data                             \n",
        "from nltk.corpus import twitter_samples\n",
        "# Preprocessing tools\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings in Tweeter\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import random\n",
        "import string\n",
        "# BOW Models\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# CBOW Model\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from nltk.corpus import gutenberg\n",
        "from string import punctuation\n",
        "import keras\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download stopwords\n",
        "nltk.download('stopwords')\n",
        "# download punctuation\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3SdJx-ZC_eG",
        "outputId": "5287cf80-ba71-43fe-8a67-91ff90344b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download twitter data"
      ],
      "metadata": {
        "id": "ahJ2waAw_je0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "446-H1Ho_oiZ",
        "outputId": "73cbc6e4-4cab-47cb-e029-25987db1e571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Twitter data has two subsets positive and negative tweets."
      ],
      "metadata": {
        "id": "lPgqxVsf_wra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "66pXQaVC_gZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print how many tweets are in each subset (use ``len()``) and print a random sample tweet.\n",
        "\n",
        "To learn more about using colors in notebook see [https://newbedev.com/shell-ascii-colour-codes-code-example](https://newbedev.com/shell-ascii-colour-codes-code-example)"
      ],
      "metadata": {
        "id": "c037xRepABjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of positive tweets: ', len(all_positive_tweets))\n",
        "print('Number of negative tweets: ', len(all_negative_tweets))\n",
        "# print a sample of positive tweet in greeen - '\\033[92m'\n",
        "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
        "# print a sample of negative tweet in red '\\033[91m'\n",
        "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASGhxPnHAXc-",
        "outputId": "c72ff0f5-07b7-42e5-ac24-67d8e26fbf37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive tweets:  5000\n",
            "Number of negative tweets:  5000\n",
            "\u001b[92m@spencerlist Yay another vine I love it. PLEASE can you do more vines?With peyton too :)\n",
            "\u001b[91m@electricgujtars not u i love u :(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 2 Preprocessing\n",
        "\n",
        "In this section, yoou will work first with a sample tweet to help you assemble your pipeline, then you will create a function ``process_tweet()`` and apply it to *all_positive_tweets*.\n",
        "\n",
        "Your preprocessing steps should include the following tasks:\n",
        "\n",
        "- Removing urls, hashtags, handles\n",
        "- Lowercasing\n",
        "- Removing stop words and punctuation\n",
        "- Removing repeated characters (e.g. soooo)\n",
        "- Tokenizing string\n",
        "- Stemming\n"
      ],
      "metadata": {
        "id": "KpVJN_DtBNF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1. Substrings - 5pts\n",
        "\n",
        "- You need to remove some substrings like the hashtag and hyperlinks. \n",
        "- You will use ``re.sub()`` method and regular expressions to remove **URL** and **hashtags** and substitute them by an empty character ''\n",
        "  \n",
        "  - Use ``tweet_sample`` as input\n",
        "\n",
        "  - Create ``clean_tweet`` as output\n",
        "\n",
        "  - Replace repeating characters - review Sarkar Ch.3. Yuo can use provided solutions in the Chapter. Make sure you understand how it is done.\n",
        "<hr>\n",
        "- Hint1: assume that url can be only http:// or https://\n",
        "\n",
        "- Hint2: url does not have spaces (e.g. hypothetical url http://some.ch//WjdiW4v)\n",
        "\n",
        "- See examples of re.sub() usage: https://www.kite.com/python/answers/how-to-use-re.sub()-in-python"
      ],
      "metadata": {
        "id": "Z_T0w6JuD_zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41KVn4ZZdITg",
        "outputId": "51c965e6-2a05-4a2a-e979-2fe44a44e8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tweet_example for testing\n",
        "tweet_example = \"This is a soo coooool #dummysmiley: :-) :-P <3 http://some.ch//WjdiW4v and some arrows < > -> <-- @remy: This is waaaaayyyy too much for you!!!!!!\"\n",
        "print('\\033[92m' + tweet_example)\n",
        "\n",
        "\n",
        "\n",
        "# remove hyperlinks\n",
        "\n",
        "\n",
        "# remove hashtags\n",
        "\n",
        "\n",
        "\n",
        "# remove repeated characters - this function creates an array. Use join to concatenate array. \n",
        "\n",
        "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
        "\n",
        "# join \n",
        "\n",
        "# print clean tweet\n",
        "  \n"
      ],
      "metadata": {
        "id": "2kISu6roD8GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2 Tokenizer 45pts\n",
        "\n",
        "In this homework you will use Tweeter tokenizer instead of Word tokenizer from NLTK. Look at the examples below. What are their main differences and what are their default values?\n",
        "```\n",
        "word_tokenize (\n",
        "text,\n",
        "language: str=str,\n",
        "preserve_line: bool=False\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "TweetTokenizer (\n",
        "preserve_case: bool=True,\n",
        "reduce_len: bool=False,\n",
        "strip_handles: bool=False\n",
        ")\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JrsYCfnFIgWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings in Tweeter\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "print(tt.tokenize(tweet_example))\n",
        "print(word_tokenize(tweet_example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPAJxTeUI552",
        "outputId": "0e0b6d1f-af19-4c8b-c075-8c8bb3cc04ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'http://some.ch//WjdiW4v', 'and', 'some', 'arrows', '<', '>', '->', '<--', '@remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
            "['This', 'is', 'a', 'cooool', '#', 'dummysmiley', ':', ':', '-', ')', ':', '-P', '<', '3', 'http', ':', '//some.ch//WjdiW4v', 'and', 'some', 'arrows', '<', '>', '-', '>', '<', '--', '@', 'remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1. Name Differences\n",
        "\n",
        " What are the main differences and what are the default values for tweet tokenizer and word tokenizer in NLTK?\n",
        "\n",
        "**Type your answer here:** "
      ],
      "metadata": {
        "id": "-3B9ygR-Qbn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2. Tokenize Strings\n",
        "\n",
        "- Use TweetTokenizer() method\n",
        "  -  Set ``stripping handles`` as true\n",
        "  - Set ``preserving`` case as false\n",
        "  - Set ``reducing length`` as false\n",
        "\n",
        "- Use ``clean_tweet`` as input\n",
        "- Create ``tweet_tokens`` as output"
      ],
      "metadata": {
        "id": "6ptt_ry6RYDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate tokenizer class\n",
        "\n",
        "\n",
        "# tokenize tweet\n",
        "\n",
        "# print tweet tokens\n"
      ],
      "metadata": {
        "id": "GFjHoRm9RWuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 3. Stopwords and punct"
      ],
      "metadata": {
        "id": "ESNllnjrWoNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english')\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03SKodwSVLnx",
        "outputId": "27eabf3d-d1ef-493d-eef0-4d161b061351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove stopswords and punctuation from ```tweet_tokens```"
      ],
      "metadata": {
        "id": "fX9VPj8dQq9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop words and punctutation\n",
        "\n",
        "# print tweet"
      ],
      "metadata": {
        "id": "hG165KnfWudZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 4. Stemming\n",
        "\n",
        "- Use PorterStemmer()\n",
        "- Create an array as your output"
      ],
      "metadata": {
        "id": "xCPjIG9CW7rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate stemming class with Porter stemmer\n",
        "\n",
        "# apply stemmer\n",
        "\n",
        "# print stemmed tweet\n"
      ],
      "metadata": {
        "id": "NENzmMC2XAHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 5 Function normalize_tweet()\n",
        "\n",
        "- Consult Sarkar (Book Ch3 and Github Ch3) and our course book practice [https://obscrivn.github.io/mynewbook/practice2.html](https://obscrivn.github.io/mynewbook/practice2.html) and [https://obscrivn.github.io/mynewbook/practice.html](https://obscrivn.github.io/mynewbook/practice.html) and Ch4a github.\n",
        "- Create a new function ``normalize_tweet()`` similar to ``normalize_document`` from Ch4  but you must make some additional changes\n",
        "- Your function must include the following new tasks:\n",
        "  - removing url and hashtags\n",
        "  - removing digits\n",
        "  - using ``TweetTokenizer()`` instead of ``word_tokenize``\n",
        "  - correcting repeated characters\n",
        "  - using replacement patterns and class ``RegexpReplacer`` to expand contractions (e.g. they've)\n",
        "- Apply ``tokenize_tweet()`` function on ``all_positive_tweets``. Do not print all results (you have 5000 tweets!)\n",
        "- Select one random tweet, apply the function and print the original tweet and processed tokenized tweet\n",
        "\n",
        "\n",
        "Note 1 - your twitter corpus has been already split by tweets - it is an array. Since tweets are usually short, assume we do not need to use sent_tokenize and treat each tweet as one sentence.\n",
        "\n",
        "Note 2 - think about the ordering of preprocessing. First, you should apply any string cleaning (ex. re.sub(), re.lower()...). Then, you may want to apply replacement patterns. Then you can tokenize, apply repeated characters removals, stemming, stop words removal etc\n",
        "\n",
        "Note 3 - If you are using normalize_document structure (not normalize_corpus -> make sure you understand the difference), do not forget to use np.vectorize  - see Ch4a github.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Doka45O5XfHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function normalize_tweet()"
      ],
      "metadata": {
        "id": "lAL9UNryiKHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply this function to tweet_example \n",
        "tweet_example = \"This is a soo coooool #dummysmiley: :-) :-P <3 http://some.ch//WjdiW4v and some arrows < > -> <-- @remy: This is waaaaayyyy too much for you!!!!!!\"\n",
        "\n",
        "#print original and normalized tweet"
      ],
      "metadata": {
        "id": "hGOVRMkltlzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 3 Feature Extraction 50pts"
      ],
      "metadata": {
        "id": "xQ7MIpHGl-x1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1. Count Vectorizer 15pts\n",
        "\n",
        "- Apply count vectorizer from sklearn to normalized positive tweet corpus\n",
        "- Convert maatrix to data frame\n",
        "- Add vocabulary names\n",
        "- Print only 6 first rows of data frame"
      ],
      "metadata": {
        "id": "iiMiAIfXmCxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Vectorizer\n",
        "\n",
        "# create matrix\n",
        "\n",
        "# convert matrix to dataframe\n",
        "\n",
        "# add vocabulary names\n",
        "\n",
        "# print only 5 first rows of data frame"
      ],
      "metadata": {
        "id": "OI5J0JHel5rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2. TF-IDF - 20pts\n",
        "\n",
        "- Apply TF-IDF with smoothing"
      ],
      "metadata": {
        "id": "cs1M9tEEnk1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "\n",
        "# create matrix\n",
        "\n",
        "# convert to dataframe\n",
        "\n",
        "# add vocabulary names\n",
        "\n",
        "# print the first 5 rows of data frame"
      ],
      "metadata": {
        "id": "uRx0lEV3uQoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3. CBOW - 25pts\n",
        "\n",
        "Note - this section is based on your week4 CBOW practice\n",
        "\n",
        "- Use your normalized twitter corpus\n",
        "- Use the same window and embedding size as in your week 4 CBOW practice\n",
        "- Create model and print model summary [see practice for details]\n",
        "- Create padded docs\n",
        "- Use function generate context pairs from week 4 cbow practice\n",
        "- Extract weights\n",
        "- Print the first 5 rows\n",
        "\n",
        "\n",
        "Note1 - you may need to change runtime to GPU\n",
        "\n",
        "Note2 - you can use a smaller size of twitter data (~100 tweets) to save you training time"
      ],
      "metadata": {
        "id": "NQuHQGSQoSU1"
      }
    }
  ]
}